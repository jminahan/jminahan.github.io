---
layout: post
title: Learning-- CS231n Assignment 1.1:KNN
date: 2018-03-02 14:36:00
categories: ruby
short_description: Working through the Spring 2018 CS231n Course
---
In this 'series' of posts, I'll be walking through my code for the <a href="http://cs231n.stanford.edu/index.html">CS231n</a> assigments.  This post will focus on the KNN questions in the first assignment.

Assigment Introduction: The majority of the focus on this assigment was to discover increasingly vectorized methods for calculating the distances between a single point compared to a "trained" or "learned" set when performing an instance of the KNN algorithm.  The first step was to consider the least efficient manner, looping over all unknown points and all known points.  Next, students took advantage of the vector capabilities of NumPy in order to reduce that to one loop.  Finally, with some simple algebra, you could get down to a completely vectorized version of the algorithm.

The two-looped version is pretty simple.

<code>
	num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train))
    for i in xrange(num_test):
      for j in xrange(num_train):
        dists[i][j] = np.sqrt(np.sum(np.square(X[i,:]-self.X_train[j,:])))
    return dists
</code>

First, it is important to define what each of the variables are (though in this case, the variables are handily-named).

<ul>
	<li><strong>X</strong> is a (n,3072) array, where each column in a list of 3072 pixel values of a 32x32 image spread across 3 different color channels.  There are n columns where n is the number of test images you have trying to classify</li>
	<li><strong>X_train_shape</strong> is a (k, 3072) array in the same structure of <strong>X</strong>.  In this case, though, k is the number of training samples you have.</li>
	<li><strong>dists</strong> is an (n,k) array where each index is the distance of that unknown category to each training sample</li>
</ul>

So, with that information, you can see that the loop is essentially saying "for each instance of the test set, do {for each instance of the train set, do { calculate and save the distance between the points } }".

One thing that is important to note is that the formula used to calculate distance is sqrt(a-b)^2.  This will become helpful in the fully vectorized version.

The one-looped version, too, was also pretty simple.

<code>
	num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train))
    for i in xrange(num_test):
      dists[i] = np.sqrt(np.sum(np.square((X[i]-self.X_train)), axis=1))
    return dists
</code>

The variables remain the same, and the intent of the loop stays the same.  All that's different is that this time, we are calculating the distance to <strong>ALL</strong> training points simultaneously.  This helps to cut down on the running-time of the algorithm.

Finally, we get to the trickier no-loop version.

<code>
	num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train)) 
	dists = np.sqrt(np.sum(np.square(X),axis=1,keepdims=True) - 2*np.dot(X, np.transpose(self.X_train)) + np.sum(np.square(self.X_train), axis=1))
</code>

To me, this was a little unintuitive of a transition at first.  Really, though, it is only taking advantage of the algorithmic expansion of the original distance formula.

As I said, the distance formula used (called L2 throughout the lectures) is sqrt(a-b)^2.  If you expand the (a-b)^2 term, you end up with (a^2 - 2ab + b^2).  This expression can be calculated without any loops.

Overall, I found this section of the homework pretty easy.  There were other sub-problems that students were assigned (such as how to find the best hyper-parameters or how to find the accuracy of a classifier), but realistically, that was just looping through information you already had.  The real complexity of the assignment was in implementing the KNN algorithm (as shown above).